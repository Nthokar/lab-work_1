behaviors:      # окружение агента
  RollerBall:   #
    trainer_type: ppo   #type of learning technique also avalible sac, poca
    hyperparameters:  
      batch_size: 10    #Типовой диапазон:
                        #(непрерывный - PPO): 512 - 5120;
                        #(непрерывный - SAC): 128 - 1024;
                        #(Дискретный, PPO и SAC): 32–512.

                        #Количество опытов в каждой итерации градиентного спуска.
                        #Это всегда должно быть в несколько раз меньше, чем размер буфера.
                        #Если вы используете непрерывные действия, это значение должно быть большим (порядка 1000 с).
                        #Если вы используете только дискретные действия, это значение должно быть меньше (порядка 10 с).

      buffer_size: 100  #Типовой диапазон:
                        #PPO: 2048 - 409600;
                        #САК: 50000 - 1000000

                        #PPO: количество опыта, которые необходимо собрать перед обновлением модели поведения. Соответствует тому,
                        #сколько опыта должно быть собрано, прежде чем мы будем изучать или обновлять модель. Это значение должно быть
                        #в несколько раз больше, чем batch_size. Обычно больший batch_size соответствует более стабильным обновлениям обучения.
                        #SAC: максимальный размер буфера опыта — примерно в тысячи раз больше, чем ваши эпизоды,
                        #чтобы SAC мог учиться как на старом, так и на новом опыте.

      learning_rate: 3.0e-4
                        #(по умолчанию = 3e-4)
                        #Типичный диапазон: 1e-5 - 1e-3
                        #Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска.
                        #Обычно это значение следует уменьшать, если обучение нестабильно, а вознаграждение не увеличивается постоянно. 

      beta: 5.0e-4      #(по умолчанию = 5.0e-3)
                        #Типичный диапазон: 1e-4 - 1e-2
                        #Сила энтропийной регуляризации, которая делает поведение «более случайным».
                        #Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения.
                        #Увеличение этого параметра обеспечит выполнение большего количества случайных действий.
                        #Это должно быть скорректировано таким образом, чтобы энтропия (измеряемая с помощью TensorBoard)
                        #медленно уменьшалась вместе с увеличением вознаграждения. Если энтропия падает слишком быстро, увеличьте бета.
                        #Если энтропия падает слишком медленно, уменьшите бета.

      epsilon: 0.2      #(по умолчанию = 0,2)
                        #Типичный диапазон: 0,1–0,3
                        #Влияет на скорость изменения поведения во время обучения. Соответствует допустимому порогу расхождения между старой
                        #и новой политикой при обновлении градиентного спуска. Установка небольшого значения этого параметра приведет к более
                        #стабильным обновлениям, но также замедлит процесс обучения.

      lambd: 0.99       #(по умолчанию = 0,95)
                        #Типичный диапазон: 0,9–0,95
                        #Параметр регуляризации (лямбда), используемый при расчете обобщенной оценки преимущества (GAE).
                        #Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости.
                        #Низкие значения соответствуют большему полаганию на текущую оценку ценности (что может быть высоким смещением), а высокие значения
                        #соответствуют большему полаганию на фактические вознаграждения, полученные в среде (что может быть высокой дисперсией). Параметр обеспечивает
                        #компромисс между ними, и правильное значение может привести к более стабильному процессу обучения.

      num_epoch: 3      #(по умолчанию = 3)
                        #Типичный диапазон: 3–10
                        #Количество проходов через буфер опыта при оптимизации градиентного спуска. Чем больше размер партии,
                        #тем больше это допустимо. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения.

      learning_rate_schedule: linear
                        #(по умолчанию = линейный для PPO и постоянный для SAC)
                        #Определяет, как скорость обучения изменяется с течением времени. Для PPO мы рекомендуем снижать скорость обучения до значения max_steps, чтобы обучение сходилось более стабильно.
                        #Однако в некоторых случаях (например, при обучении в течение неизвестного времени) эту функцию можно отключить. Для SAC мы рекомендуем поддерживать скорость обучения постоянной,
                        #чтобы агент мог продолжать обучение до тех пор, пока его функция Q не сойдется естественным образом.
                        #linear линейно затухает Learning_rate, достигая 0 при max_steps, в то время как Constant сохраняет скорость обучения постоянной для всего тренировочного прогона.
    network_settings:
      normalize: false  #(по умолчанию = false)
                        #Применяется ли нормализация к входным данным векторного наблюдения. Эта нормализация основана на скользящем среднем и дисперсии векторного наблюдения.
                        #Нормализация может быть полезна в случаях со сложными задачами непрерывного управления, но может быть вредна для более простых задач дискретного управления.

      hidden_units: 128 #(по умолчанию = 128)
                        #Типичный диапазон: 32–512
                        #Количество юнитов в скрытых слоях нейронной сети. Соответствуют количеству единиц в каждом полносвязном слое нейронной сети.
                        #Для простых задач, где правильное действие представляет собой простую комбинацию входных данных наблюдения, это значение должно быть небольшим.
                        #Для задач, где действие представляет собой очень сложное взаимодействие между переменными наблюдения, это значение должно быть больше.

      num_layers: 2     #(по умолчанию = 2)
                        #Типичный диапазон: 1–3
                        #Количество скрытых слоев в нейронной сети. Соответствует количеству скрытых слоев после ввода наблюдения или после кодирования CNN визуального наблюдения.
                        #Для простых задач меньше слоев, скорее всего, будут обучать быстрее и эффективнее. Для более сложных задач управления может потребоваться больше слоев.
    reward_signals:
      extrinsic:
        gamma: 0.99     #(по умолчанию = 0,99)
                        #Типичный диапазон: 0,8–0,995
                        #Фактор скидки для будущих вознаграждений, поступающих из окружающей среды. Это можно рассматривать как то, как далеко в будущем агент должен заботиться
                        #о возможных вознаграждениях. В ситуациях, когда агент должен действовать в настоящем, чтобы подготовиться к вознаграждению в отдаленном будущем,
                        #это значение должно быть большим. В случаях, когда вознаграждение является более немедленным, оно может быть меньше. Должно быть строго меньше 1.

        strength: 1.0   #(по умолчанию = 1.0)
                        #Типичный диапазон: 1,00
                        #Коэффициент, на который умножается вознаграждение, данное средой. Типичные диапазоны будут варьироваться в зависимости от сигнала вознаграждения.

    max_steps: 500000   #(по умолчанию = 500000)
                        #Типичный диапазон: 5e5 - 1e7
                        #Общее количество шагов (т. е. собранных наблюдений и предпринятых действий), которые необходимо выполнить в среде
                        #(или во всех средах при параллельном использовании нескольких) перед завершением процесса обучения. Если в вашей среде есть несколько агентов с одинаковым именем поведения,
                        #все шаги, предпринятые этими агентами, будут учитываться в одном и том же счетчике max_steps.

    time_horizon: 64    #(по умолчанию = 64)
                        #Типовой диапазон: 32 - 2048
                        #Сколько шагов опыта необходимо собрать для каждого агента, прежде чем добавить его в буфер опыта. Когда этот предел достигается до конца эпизода,
                        #оценка значения используется для прогнозирования общего ожидаемого вознаграждения из текущего состояния агента. Таким образом, этот параметр
                        #является компромиссом между менее предвзятой, но более высокой оценкой дисперсии (длинный временной горизонт) и более предвзятой, но менее разнообразной оценкой
                        #(короткий временной горизонт). В тех случаях, когда в эпизоде ​​есть частые награды или эпизоды непомерно велики, более идеальным может быть меньшее количество.
                        #Это число должно быть достаточно большим, чтобы охватить все важные действия в последовательности действий агента.

    summary_freq: 10000 #(по умолчанию = 50000)
                        #Количество опытов, которое необходимо собрать перед созданием и отображением статистики обучения.
                        #Это определяет детализацию графиков в Tensorboard.