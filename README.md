# lab-work_1
 # АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил(а):
- Владимир Андреевич Жирнов
- РИ210947
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;
Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.


Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Реализовать систему машинного обучения в связке Python - Google-Sheets – Unity. При выполнении задания можно использовать видео-
материалы и исходные данные, предоставленные преподавателями курса.

- Создайте новый пустой 3D проект на Unity.
- Скачайте папку с ML агентом. Вы найдете ее в облаке с исходными файлами к лабораторной работе – ml-agents-release_19.
- В созданный проект добавьте ML Agent, выбрав Window - Package Manager - Add Package from disk. Последовательно добавьте .json – файлы:
	o ml-agents-release_19 / com,unity.ml-agents / package.json
	o ml-agents-release_19 / com,unity.ml-agents.extensions / package.json
	
- Если все сделано правильно, то во вкладке с компонентами
  (Components) внутри Unity вы увидите строку ML Agent.
- Далее запускаем Anaconda Prompt для возможности запуска команд через консоль.
- Далее пишем серию команд для создания и активации нового ML-агента,
   а также для скачивания необходимых библиотек:
	o mlagents 0.28.0;
	o torch 1.7.1;
	
Скриншот слздания MLAgent

![image](https://github.com/Nthokar/lab-work_1/tree/lab_work_3/screenshots/1.jpg)

[(Скриншот установки библиотеки mlagent](https://github.com/Nthokar/lab-work_1/tree/lab_work_3/screenshots/2.jpg)

[(Скриншот установки библиотеки torch и перехода в дерикторию проекта)](https://github.com/Nthokar/lab-work_1/tree/lab_work_3/screenshots/3.jpg)

- Создайте на сцене плоскость, куб и сферу так, как показано на рисунке
  ниже. Создайте простой C# скрипт-файл и подключите его к сфере:
  
- В скрипт-файл RollerAgent.cs добавьте код, опубликованный в
  материалах лабораторных работ – по ссылке. Объекту «сфера» добавить компоненты Rigidbody, Decision Requester,
  Behavior Parameters и настройте их так, как показано на рисунке.
  
- В корень проекта добавьте файл конфигурации нейронной сети,
  доступный в папке с файлами проекта по ссылке.
  Запустите работу ml-агента
  
  [(Скриншот работы ml-агента)](https://github.com/Nthokar/lab-work_1/tree/lab_work_3/screenshots/10.jpg)
  
- Вернитесь в проект Unity, запустите сцену, проверьте работу ML-Agent’a.
	Сделайте 3, 9, 27 копий модели «Плоскость-Сфера-Куб», запустите
	симуляцию сцены и наблюдайте за результатом обучения модели.
	
	[(Скриншот работы 3-х копий)](https://github.com/Nthokar/lab-work_1/tree/lab_work_3/screenshots/7.jpg)
	
	[(Скриншот работы 9-х копий)](https://github.com/Nthokar/lab-work_1/tree/lab_work_3/screenshots/8.jpg)
	
	[(Скриншот работы 27-х копий)](https://github.com/Nthokar/lab-work_1/tree/lab_work_3/screenshots/9.jpg)
	
	После завершения обучения проверьте работу модели.
	
	Сделайте выводы.
	
	После завершения обучения нейронная сеть научилась быстро и безошибочно сближаться с целью
  
## Задание 2
### Подробно опишите каждую строку файла конфигурации
нейронной сети, доступного в папке с файлами проекта по ссылке. Самостоятельно
найдите информацию о компонентах Decision Requester, Behavior Parameters,
добавленных на сфере.

```
behaviors:      # окружение агента
  RollerBall:   #
    trainer_type: ppo   #type of learning technique also avalible sac, poca
    hyperparameters:  
      batch_size: 10    #Типовой диапазон:
                        #(непрерывный - PPO): 512 - 5120;
                        #(непрерывный - SAC): 128 - 1024;
                        #(Дискретный, PPO и SAC): 32–512.

                        #Количество опытов в каждой итерации градиентного спуска.
                        #Это всегда должно быть в несколько раз меньше, чем размер буфера.
                        #Если вы используете непрерывные действия, это значение должно быть большим (порядка 1000 с).
                        #Если вы используете только дискретные действия, это значение должно быть меньше (порядка 10 с).

      buffer_size: 100  #Типовой диапазон:
                        #PPO: 2048 - 409600;
                        #САК: 50000 - 1000000

                        #PPO: количество опыта, которые необходимо собрать перед обновлением модели поведения. Соответствует тому,
                        #сколько опыта должно быть собрано, прежде чем мы будем изучать или обновлять модель. Это значение должно быть
                        #в несколько раз больше, чем batch_size. Обычно больший batch_size соответствует более стабильным обновлениям обучения.
                        #SAC: максимальный размер буфера опыта — примерно в тысячи раз больше, чем ваши эпизоды,
                        #чтобы SAC мог учиться как на старом, так и на новом опыте.

      learning_rate: 3.0e-4
                        #(по умолчанию = 3e-4)
                        #Типичный диапазон: 1e-5 - 1e-3
                        #Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска.
                        #Обычно это значение следует уменьшать, если обучение нестабильно, а вознаграждение не увеличивается постоянно. 

      beta: 5.0e-4      #(по умолчанию = 5.0e-3)
                        #Типичный диапазон: 1e-4 - 1e-2
                        #Сила энтропийной регуляризации, которая делает поведение «более случайным».
                        #Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения.
                        #Увеличение этого параметра обеспечит выполнение большего количества случайных действий.
                        #Это должно быть скорректировано таким образом, чтобы энтропия (измеряемая с помощью TensorBoard)
                        #медленно уменьшалась вместе с увеличением вознаграждения. Если энтропия падает слишком быстро, увеличьте бета.
                        #Если энтропия падает слишком медленно, уменьшите бета.

      epsilon: 0.2      #(по умолчанию = 0,2)
                        #Типичный диапазон: 0,1–0,3
                        #Влияет на скорость изменения поведения во время обучения. Соответствует допустимому порогу расхождения между старой
                        #и новой политикой при обновлении градиентного спуска. Установка небольшого значения этого параметра приведет к более
                        #стабильным обновлениям, но также замедлит процесс обучения.

      lambd: 0.99       #(по умолчанию = 0,95)
                        #Типичный диапазон: 0,9–0,95
                        #Параметр регуляризации (лямбда), используемый при расчете обобщенной оценки преимущества (GAE).
                        #Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости.
                        #Низкие значения соответствуют большему полаганию на текущую оценку ценности (что может быть высоким смещением), а высокие значения
                        #соответствуют большему полаганию на фактические вознаграждения, полученные в среде (что может быть высокой дисперсией). Параметр обеспечивает
                        #компромисс между ними, и правильное значение может привести к более стабильному процессу обучения.

      num_epoch: 3      #(по умолчанию = 3)
                        #Типичный диапазон: 3–10
                        #Количество проходов через буфер опыта при оптимизации градиентного спуска. Чем больше размер партии,
                        #тем больше это допустимо. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения.

      learning_rate_schedule: linear
                        #(по умолчанию = линейный для PPO и постоянный для SAC)
                        #Определяет, как скорость обучения изменяется с течением времени. Для PPO мы рекомендуем снижать скорость обучения до значения max_steps, чтобы обучение сходилось более стабильно.
                        #Однако в некоторых случаях (например, при обучении в течение неизвестного времени) эту функцию можно отключить. Для SAC мы рекомендуем поддерживать скорость обучения постоянной,
                        #чтобы агент мог продолжать обучение до тех пор, пока его функция Q не сойдется естественным образом.
                        #linear линейно затухает Learning_rate, достигая 0 при max_steps, в то время как Constant сохраняет скорость обучения постоянной для всего тренировочного прогона.
    network_settings:
      normalize: false  #(по умолчанию = false)
                        #Применяется ли нормализация к входным данным векторного наблюдения. Эта нормализация основана на скользящем среднем и дисперсии векторного наблюдения.
                        #Нормализация может быть полезна в случаях со сложными задачами непрерывного управления, но может быть вредна для более простых задач дискретного управления.

      hidden_units: 128 #(по умолчанию = 128)
                        #Типичный диапазон: 32–512
                        #Количество юнитов в скрытых слоях нейронной сети. Соответствуют количеству единиц в каждом полносвязном слое нейронной сети.
                        #Для простых задач, где правильное действие представляет собой простую комбинацию входных данных наблюдения, это значение должно быть небольшим.
                        #Для задач, где действие представляет собой очень сложное взаимодействие между переменными наблюдения, это значение должно быть больше.

      num_layers: 2     #(по умолчанию = 2)
                        #Типичный диапазон: 1–3
                        #Количество скрытых слоев в нейронной сети. Соответствует количеству скрытых слоев после ввода наблюдения или после кодирования CNN визуального наблюдения.
                        #Для простых задач меньше слоев, скорее всего, будут обучать быстрее и эффективнее. Для более сложных задач управления может потребоваться больше слоев.
    reward_signals:
      extrinsic:
        gamma: 0.99     #(по умолчанию = 0,99)
                        #Типичный диапазон: 0,8–0,995
                        #Фактор скидки для будущих вознаграждений, поступающих из окружающей среды. Это можно рассматривать как то, как далеко в будущем агент должен заботиться
                        #о возможных вознаграждениях. В ситуациях, когда агент должен действовать в настоящем, чтобы подготовиться к вознаграждению в отдаленном будущем,
                        #это значение должно быть большим. В случаях, когда вознаграждение является более немедленным, оно может быть меньше. Должно быть строго меньше 1.

        strength: 1.0   #(по умолчанию = 1.0)
                        #Типичный диапазон: 1,00
                        #Коэффициент, на который умножается вознаграждение, данное средой. Типичные диапазоны будут варьироваться в зависимости от сигнала вознаграждения.

    max_steps: 500000   #(по умолчанию = 500000)
                        #Типичный диапазон: 5e5 - 1e7
                        #Общее количество шагов (т. е. собранных наблюдений и предпринятых действий), которые необходимо выполнить в среде
                        #(или во всех средах при параллельном использовании нескольких) перед завершением процесса обучения. Если в вашей среде есть несколько агентов с одинаковым именем поведения,
                        #все шаги, предпринятые этими агентами, будут учитываться в одном и том же счетчике max_steps.

    time_horizon: 64    #(по умолчанию = 64)
                        #Типовой диапазон: 32 - 2048
                        #Сколько шагов опыта необходимо собрать для каждого агента, прежде чем добавить его в буфер опыта. Когда этот предел достигается до конца эпизода,
                        #оценка значения используется для прогнозирования общего ожидаемого вознаграждения из текущего состояния агента. Таким образом, этот параметр
                        #является компромиссом между менее предвзятой, но более высокой оценкой дисперсии (длинный временной горизонт) и более предвзятой, но менее разнообразной оценкой
                        #(короткий временной горизонт). В тех случаях, когда в эпизоде ​​есть частые награды или эпизоды непомерно велики, более идеальным может быть меньшее количество.
                        #Это число должно быть достаточно большим, чтобы охватить все важные действия в последовательности действий агента.

    summary_freq: 10000 #(по умолчанию = 50000)
                        #Количество опытов, которое необходимо собрать перед созданием и отображением статистики обучения.
                        #Это определяет детализацию графиков в Tensorboard.
```

## Выводы

Абзац умных слов о том, что было сделано и что было узнано.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
